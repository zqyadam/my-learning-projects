{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# MNIST_with_CNN"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "metadata": {},
=======
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
>>>>>>> ec58ac9e310756f4cba8f4938cc77f7d9a63c29b
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 关于数据集\n",
    "\n",
    "MNIST数据集中包含了三个部分：55000个训练集（mnist.train），10000个测试集（mnist.test）和5000个验证集（mnist.validation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 绘制前10个训练集图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADTCAYAAACRDeixAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wEj1CehBe65VQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9c680377f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 3 4 6 1 8 1 0 9 8]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    image = mnist.train.images[i]\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.imshow(image.reshape([28,28]), cmap='gray_r')\n",
    "plt.show()\n",
    "print(np.argmax(mnist.train.labels[:10], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 28, 28, 1)\n",
      "(5000, 10)\n"
     ]
    }
   ],
   "source": [
    "val_features = np.array([ mnist.validation.images[i].reshape((28,28,1))  for i in range(mnist.validation.images.shape[0])])\n",
    "print(val_features.shape)\n",
    "val_labels = mnist.validation.labels\n",
    "print(val_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 构建卷积神经网络\n",
    "\n",
    "使用3个卷积层+池化层，2个全连接层构建神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 权重初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def init_weight(shape):\n",
    "    init = tf.truncated_normal(shape, mean=0,  stddev=0.1)\n",
    "    return tf.Variable(init)\n",
    "\n",
    "# 偏置初始化\n",
    "def init_bias(shape):\n",
    "    init = tf.zeros(shape = shape)\n",
    "    return tf.Variable(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**注：**这里需要注意权重初始化时的stddev，权重在初始化的时候越靠近0越好，因为当权重全部为0的时候，意味着所有的反向传播值都是一样的，也就是说神经网络没有从误差中学到任何东西，也就无法体现这些超参数的差异化；如果初始化的值远离0了，那么可能会使神经网络在初始化的时候出面对某种结果的喜好，从而影响网络的训练，这种情况可能会使网络陷入局部解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 构建各个层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 卷积层和池化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 卷积层\n",
    "def conv2d(x_tensor, outputs_num, kernel_size=(3,3), strides=(1,1,1,1) ):\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    weight = init_weight([kernel_size[0], kernel_size[1], shape[3], outputs_num])\n",
    "    bias = init_bias([outputs_num])\n",
    "    \n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weight, strides, padding='SAME' )\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    return conv_layer\n",
    "\n",
    "# 池化层\n",
    "def pooling(x_tensor, kernel_size=(2,2), strides=(1,2,2,1)):\n",
    "    pooling_layer = tf.nn.max_pool(x_tensor, ksize=[1, kernel_size[0],kernel_size[1],1], strides=strides, padding=\"SAME\")\n",
    "    return pooling_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 扁化层\n",
    "扁化层是用来把最后一个卷积池化层的输出转换成单一向量，用来作为后面全连接层的输入层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def flatten(x_tensor):\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    x_tensor = tf.reshape(x_tensor, [-1, shape[1]*shape[2]*shape[3]])\n",
    "    return x_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, num_outputs, activation=tf.nn.relu):\n",
    "    return tf.layers.dense(x_tensor, num_outputs, activation=activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 构建全局神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv_net(x, keep_probility):\n",
    "    \n",
    "    conv_layer1 = conv2d(x, 64)\n",
    "    pooling_layer1 = pooling(conv_layer1)\n",
    "    \n",
    "    conv_layer2 = conv2d(pooling_layer1, 128)\n",
    "    pooling_layer2 = pooling(conv_layer2)\n",
    "\n",
    "    conv_layer3 = conv2d(pooling_layer2, 256)\n",
    "    pooling_layer2 = pooling(conv_layer3)\n",
    "    \n",
    "    flatten_layer = flatten(conv_layer3)\n",
    "    \n",
    "    fully_conn_layer1 = fully_conn(flatten_layer, 1024)\n",
    "    fully_conn_layer1 = tf.nn.dropout(fully_conn_layer1, keep_prob = keep_probility)\n",
    "    fully_conn_layer2 = fully_conn(fully_conn_layer1, 512)\n",
    "    fully_conn_layer2 = tf.nn.dropout(fully_conn_layer2, keep_prob = keep_probility)\n",
    "    \n",
    "    output = fully_conn(fully_conn_layer2, 10, activation=None)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name=\"x\")\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10], name=\"y\")\n",
    "keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "logits = conv_net(x, keep_prob)\n",
    "logits = tf.identity(logits, name='logits')\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32),name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_nn(session, optimizer, feature_batch, label_batch, keep_probility):\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probility})\n",
    "\n",
    "def print_status(session, feature_batch, label_batch, cost, accuracy):\n",
    "    loss = session.run(cost, feed_dict={ x: feature_batch, y: label_batch , keep_prob: 1 })\n",
    "    \n",
    "    acc = []\n",
    "    batch_size=4096\n",
    "    batches = mnist.validation.images.shape[0] // batch_size +1\n",
    "    for i in range(1, batches+1):\n",
    "        val_batch = mnist.validation.next_batch(batch_size)\n",
    "        val_features_batch = val_batch[0].reshape( [-1, 28,28,1])\n",
    "        val_labels_batch = val_batch[1]\n",
    "        acc.append( session.run(accuracy, feed_dict={x : val_features_batch, y: val_labels_batch, keep_prob:1}))\n",
    "    acc = np.mean(acc)\n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss,acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size=4096\n",
    "keep_probility = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, MNIST Batch 1:  Loss:     4.4824 Validation Accuracy: 0.165527\n",
      "Epoch  1, MNIST Batch 2:  Loss:     5.1528 Validation Accuracy: 0.106567\n",
      "Epoch  1, MNIST Batch 3:  Loss:     2.8640 Validation Accuracy: 0.365479\n",
      "Epoch  1, MNIST Batch 4:  Loss:     2.2640 Validation Accuracy: 0.469727\n",
      "Epoch  1, MNIST Batch 5:  Loss:     1.8971 Validation Accuracy: 0.487793\n",
      "Epoch  1, MNIST Batch 6:  Loss:     1.7466 Validation Accuracy: 0.513916\n",
      "Epoch  1, MNIST Batch 7:  Loss:     1.6991 Validation Accuracy: 0.572876\n",
      "Epoch  1, MNIST Batch 8:  Loss:     1.5845 Validation Accuracy: 0.621948\n",
      "Epoch  1, MNIST Batch 9:  Loss:     1.4153 Validation Accuracy: 0.653320\n",
      "Epoch  1, MNIST Batch 10:  Loss:     1.2003 Validation Accuracy: 0.713623\n",
      "Epoch  1, MNIST Batch 11:  Loss:     0.9304 Validation Accuracy: 0.760254\n",
      "Epoch  1, MNIST Batch 12:  Loss:     0.7153 Validation Accuracy: 0.803223\n",
      "Epoch  1, MNIST Batch 13:  Loss:     0.4991 Validation Accuracy: 0.836426\n",
      "Epoch  1, MNIST Batch 14:  Loss:     0.4600 Validation Accuracy: 0.838623\n",
      "Epoch  2, MNIST Batch 1:  Loss:     0.4235 Validation Accuracy: 0.866333\n",
      "Epoch  2, MNIST Batch 2:  Loss:     0.3633 Validation Accuracy: 0.883057\n",
      "Epoch  2, MNIST Batch 3:  Loss:     0.3342 Validation Accuracy: 0.890991\n",
      "Epoch  2, MNIST Batch 4:  Loss:     0.3051 Validation Accuracy: 0.903809\n",
      "Epoch  2, MNIST Batch 5:  Loss:     0.2638 Validation Accuracy: 0.920166\n",
      "Epoch  2, MNIST Batch 6:  Loss:     0.2721 Validation Accuracy: 0.929688\n",
      "Epoch  2, MNIST Batch 7:  Loss:     0.2425 Validation Accuracy: 0.936523\n",
      "Epoch  2, MNIST Batch 8:  Loss:     0.2491 Validation Accuracy: 0.932007\n",
      "Epoch  2, MNIST Batch 9:  Loss:     0.2206 Validation Accuracy: 0.939941\n",
      "Epoch  2, MNIST Batch 10:  Loss:     0.1947 Validation Accuracy: 0.948120\n",
      "Epoch  2, MNIST Batch 11:  Loss:     0.2022 Validation Accuracy: 0.949219\n",
      "Epoch  2, MNIST Batch 12:  Loss:     0.1882 Validation Accuracy: 0.948120\n",
      "Epoch  2, MNIST Batch 13:  Loss:     0.1837 Validation Accuracy: 0.950806\n",
      "Epoch  2, MNIST Batch 14:  Loss:     0.1660 Validation Accuracy: 0.955444\n",
      "Epoch  3, MNIST Batch 1:  Loss:     0.1554 Validation Accuracy: 0.958008\n",
      "Epoch  3, MNIST Batch 2:  Loss:     0.1493 Validation Accuracy: 0.959839\n",
      "Epoch  3, MNIST Batch 3:  Loss:     0.1327 Validation Accuracy: 0.961914\n",
      "Epoch  3, MNIST Batch 4:  Loss:     0.1235 Validation Accuracy: 0.962402\n",
      "Epoch  3, MNIST Batch 5:  Loss:     0.1215 Validation Accuracy: 0.962646\n",
      "Epoch  3, MNIST Batch 6:  Loss:     0.1350 Validation Accuracy: 0.965820\n",
      "Epoch  3, MNIST Batch 7:  Loss:     0.1192 Validation Accuracy: 0.967407\n",
      "Epoch  3, MNIST Batch 8:  Loss:     0.1031 Validation Accuracy: 0.968750\n",
      "Epoch  3, MNIST Batch 9:  Loss:     0.1118 Validation Accuracy: 0.969727\n",
      "Epoch  3, MNIST Batch 10:  Loss:     0.1165 Validation Accuracy: 0.970825\n",
      "Epoch  3, MNIST Batch 11:  Loss:     0.0950 Validation Accuracy: 0.972534\n",
      "Epoch  3, MNIST Batch 12:  Loss:     0.0875 Validation Accuracy: 0.973022\n",
      "Epoch  3, MNIST Batch 13:  Loss:     0.0837 Validation Accuracy: 0.976440\n",
      "Epoch  3, MNIST Batch 14:  Loss:     0.0828 Validation Accuracy: 0.974854\n",
      "Epoch  4, MNIST Batch 1:  Loss:     0.0911 Validation Accuracy: 0.976074\n",
      "Epoch  4, MNIST Batch 2:  Loss:     0.0848 Validation Accuracy: 0.975830\n",
      "Epoch  4, MNIST Batch 3:  Loss:     0.0813 Validation Accuracy: 0.977051\n",
      "Epoch  4, MNIST Batch 4:  Loss:     0.0670 Validation Accuracy: 0.977173\n",
      "Epoch  4, MNIST Batch 5:  Loss:     0.0683 Validation Accuracy: 0.978516\n",
      "Epoch  4, MNIST Batch 6:  Loss:     0.0692 Validation Accuracy: 0.979614\n",
      "Epoch  4, MNIST Batch 7:  Loss:     0.0670 Validation Accuracy: 0.978027\n",
      "Epoch  4, MNIST Batch 8:  Loss:     0.0630 Validation Accuracy: 0.977905\n",
      "Epoch  4, MNIST Batch 9:  Loss:     0.0625 Validation Accuracy: 0.979004\n",
      "Epoch  4, MNIST Batch 10:  Loss:     0.0593 Validation Accuracy: 0.981323\n",
      "Epoch  4, MNIST Batch 11:  Loss:     0.0549 Validation Accuracy: 0.980957\n",
      "Epoch  4, MNIST Batch 12:  Loss:     0.0496 Validation Accuracy: 0.981934\n",
      "Epoch  4, MNIST Batch 13:  Loss:     0.0507 Validation Accuracy: 0.982544\n",
      "Epoch  4, MNIST Batch 14:  Loss:     0.0435 Validation Accuracy: 0.982910\n",
      "Epoch  5, MNIST Batch 1:  Loss:     0.0515 Validation Accuracy: 0.983521\n",
      "Epoch  5, MNIST Batch 2:  Loss:     0.0505 Validation Accuracy: 0.984131\n",
      "Epoch  5, MNIST Batch 3:  Loss:     0.0468 Validation Accuracy: 0.983154\n",
      "Epoch  5, MNIST Batch 4:  Loss:     0.0561 Validation Accuracy: 0.984985\n",
      "Epoch  5, MNIST Batch 5:  Loss:     0.0506 Validation Accuracy: 0.983765\n",
      "Epoch  5, MNIST Batch 6:  Loss:     0.0449 Validation Accuracy: 0.983765\n",
      "Epoch  5, MNIST Batch 7:  Loss:     0.0549 Validation Accuracy: 0.984131\n",
      "Epoch  5, MNIST Batch 8:  Loss:     0.0532 Validation Accuracy: 0.984497\n",
      "Epoch  5, MNIST Batch 9:  Loss:     0.0424 Validation Accuracy: 0.986084\n",
      "Epoch  5, MNIST Batch 10:  Loss:     0.0366 Validation Accuracy: 0.985229\n",
      "Epoch  5, MNIST Batch 11:  Loss:     0.0416 Validation Accuracy: 0.986206\n",
      "Epoch  5, MNIST Batch 12:  Loss:     0.0414 Validation Accuracy: 0.986328\n",
      "Epoch  5, MNIST Batch 13:  Loss:     0.0390 Validation Accuracy: 0.987793\n",
      "Epoch  5, MNIST Batch 14:  Loss:     0.0419 Validation Accuracy: 0.988159\n",
      "Epoch  6, MNIST Batch 1:  Loss:     0.0309 Validation Accuracy: 0.985718\n",
      "Epoch  6, MNIST Batch 2:  Loss:     0.0447 Validation Accuracy: 0.986572\n",
      "Epoch  6, MNIST Batch 3:  Loss:     0.0474 Validation Accuracy: 0.985596\n",
      "Epoch  6, MNIST Batch 4:  Loss:     0.0307 Validation Accuracy: 0.985962\n",
      "Epoch  6, MNIST Batch 5:  Loss:     0.0398 Validation Accuracy: 0.986694\n",
      "Epoch  6, MNIST Batch 6:  Loss:     0.0278 Validation Accuracy: 0.987183\n",
      "Epoch  6, MNIST Batch 7:  Loss:     0.0323 Validation Accuracy: 0.988281\n",
      "Epoch  6, MNIST Batch 8:  Loss:     0.0320 Validation Accuracy: 0.988159\n",
      "Epoch  6, MNIST Batch 9:  Loss:     0.0303 Validation Accuracy: 0.988525\n",
      "Epoch  6, MNIST Batch 10:  Loss:     0.0309 Validation Accuracy: 0.989380\n",
      "Epoch  6, MNIST Batch 11:  Loss:     0.0280 Validation Accuracy: 0.987549\n",
      "Epoch  6, MNIST Batch 12:  Loss:     0.0230 Validation Accuracy: 0.988159\n",
      "Epoch  6, MNIST Batch 13:  Loss:     0.0287 Validation Accuracy: 0.987305\n",
      "Epoch  6, MNIST Batch 14:  Loss:     0.0306 Validation Accuracy: 0.988770\n",
      "Epoch  7, MNIST Batch 1:  Loss:     0.0270 Validation Accuracy: 0.986694\n",
      "Epoch  7, MNIST Batch 2:  Loss:     0.0277 Validation Accuracy: 0.987061\n",
      "Epoch  7, MNIST Batch 3:  Loss:     0.0321 Validation Accuracy: 0.988770\n",
      "Epoch  7, MNIST Batch 4:  Loss:     0.0265 Validation Accuracy: 0.990967\n",
      "Epoch  7, MNIST Batch 5:  Loss:     0.0325 Validation Accuracy: 0.990356\n",
      "Epoch  7, MNIST Batch 6:  Loss:     0.0323 Validation Accuracy: 0.988037\n",
      "Epoch  7, MNIST Batch 7:  Loss:     0.0295 Validation Accuracy: 0.988281\n",
      "Epoch  7, MNIST Batch 8:  Loss:     0.0284 Validation Accuracy: 0.988770\n",
      "Epoch  7, MNIST Batch 9:  Loss:     0.0234 Validation Accuracy: 0.989136\n",
      "Epoch  7, MNIST Batch 10:  Loss:     0.0255 Validation Accuracy: 0.989502\n",
      "Epoch  7, MNIST Batch 11:  Loss:     0.0210 Validation Accuracy: 0.988770\n",
      "Epoch  7, MNIST Batch 12:  Loss:     0.0209 Validation Accuracy: 0.991089\n",
      "Epoch  7, MNIST Batch 13:  Loss:     0.0217 Validation Accuracy: 0.990845\n",
      "Epoch  7, MNIST Batch 14:  Loss:     0.0275 Validation Accuracy: 0.990112\n",
      "Epoch  8, MNIST Batch 1:  Loss:     0.0192 Validation Accuracy: 0.989380\n",
      "Epoch  8, MNIST Batch 2:  Loss:     0.0263 Validation Accuracy: 0.990112\n",
      "Epoch  8, MNIST Batch 3:  Loss:     0.0191 Validation Accuracy: 0.991211\n",
      "Epoch  8, MNIST Batch 4:  Loss:     0.0117 Validation Accuracy: 0.991211\n",
      "Epoch  8, MNIST Batch 5:  Loss:     0.0255 Validation Accuracy: 0.990356\n",
      "Epoch  8, MNIST Batch 6:  Loss:     0.0243 Validation Accuracy: 0.989502\n",
      "Epoch  8, MNIST Batch 7:  Loss:     0.0276 Validation Accuracy: 0.990479\n",
      "Epoch  8, MNIST Batch 8:  Loss:     0.0210 Validation Accuracy: 0.990234\n",
      "Epoch  8, MNIST Batch 9:  Loss:     0.0193 Validation Accuracy: 0.988159\n",
      "Epoch  8, MNIST Batch 10:  Loss:     0.0103 Validation Accuracy: 0.989136\n",
      "Epoch  8, MNIST Batch 11:  Loss:     0.0130 Validation Accuracy: 0.991455\n",
      "Epoch  8, MNIST Batch 12:  Loss:     0.0145 Validation Accuracy: 0.990845\n",
      "Epoch  8, MNIST Batch 13:  Loss:     0.0146 Validation Accuracy: 0.991089\n",
      "Epoch  8, MNIST Batch 14:  Loss:     0.0163 Validation Accuracy: 0.991089\n",
      "Epoch  9, MNIST Batch 1:  Loss:     0.0256 Validation Accuracy: 0.990234\n",
      "Epoch  9, MNIST Batch 2:  Loss:     0.0176 Validation Accuracy: 0.990723\n",
      "Epoch  9, MNIST Batch 3:  Loss:     0.0159 Validation Accuracy: 0.990845\n",
      "Epoch  9, MNIST Batch 4:  Loss:     0.0166 Validation Accuracy: 0.990967\n",
      "Epoch  9, MNIST Batch 5:  Loss:     0.0162 Validation Accuracy: 0.990234\n",
      "Epoch  9, MNIST Batch 6:  Loss:     0.0159 Validation Accuracy: 0.989868\n",
      "Epoch  9, MNIST Batch 7:  Loss:     0.0130 Validation Accuracy: 0.989746\n",
      "Epoch  9, MNIST Batch 8:  Loss:     0.0162 Validation Accuracy: 0.989746\n",
      "Epoch  9, MNIST Batch 9:  Loss:     0.0112 Validation Accuracy: 0.990845\n",
      "Epoch  9, MNIST Batch 10:  Loss:     0.0145 Validation Accuracy: 0.991943\n",
      "Epoch  9, MNIST Batch 11:  Loss:     0.0136 Validation Accuracy: 0.991943\n",
      "Epoch  9, MNIST Batch 12:  Loss:     0.0119 Validation Accuracy: 0.991821\n",
      "Epoch  9, MNIST Batch 13:  Loss:     0.0172 Validation Accuracy: 0.990723\n",
      "Epoch  9, MNIST Batch 14:  Loss:     0.0199 Validation Accuracy: 0.989868\n",
      "Epoch 10, MNIST Batch 1:  Loss:     0.0122 Validation Accuracy: 0.991455\n",
      "Epoch 10, MNIST Batch 2:  Loss:     0.0149 Validation Accuracy: 0.990112\n",
      "Epoch 10, MNIST Batch 3:  Loss:     0.0130 Validation Accuracy: 0.990479\n",
      "Epoch 10, MNIST Batch 4:  Loss:     0.0138 Validation Accuracy: 0.991577\n",
      "Epoch 10, MNIST Batch 5:  Loss:     0.0142 Validation Accuracy: 0.991455\n",
      "Epoch 10, MNIST Batch 6:  Loss:     0.0152 Validation Accuracy: 0.991333\n",
      "Epoch 10, MNIST Batch 7:  Loss:     0.0127 Validation Accuracy: 0.990112\n",
      "Epoch 10, MNIST Batch 8:  Loss:     0.0113 Validation Accuracy: 0.991089\n",
      "Epoch 10, MNIST Batch 9:  Loss:     0.0105 Validation Accuracy: 0.991089\n",
      "Epoch 10, MNIST Batch 10:  Loss:     0.0137 Validation Accuracy: 0.990723\n",
      "Epoch 10, MNIST Batch 11:  Loss:     0.0105 Validation Accuracy: 0.989624\n",
      "Epoch 10, MNIST Batch 12:  Loss:     0.0110 Validation Accuracy: 0.990479\n",
      "Epoch 10, MNIST Batch 13:  Loss:     0.0128 Validation Accuracy: 0.991577\n",
      "Epoch 10, MNIST Batch 14:  Loss:     0.0137 Validation Accuracy: 0.992554\n",
      "Epoch 11, MNIST Batch 1:  Loss:     0.0092 Validation Accuracy: 0.990967\n",
      "Epoch 11, MNIST Batch 2:  Loss:     0.0122 Validation Accuracy: 0.991577\n",
      "Epoch 11, MNIST Batch 3:  Loss:     0.0116 Validation Accuracy: 0.990845\n",
      "Epoch 11, MNIST Batch 4:  Loss:     0.0104 Validation Accuracy: 0.991211\n",
      "Epoch 11, MNIST Batch 5:  Loss:     0.0118 Validation Accuracy: 0.990845\n",
      "Epoch 11, MNIST Batch 6:  Loss:     0.0098 Validation Accuracy: 0.990479\n",
      "Epoch 11, MNIST Batch 7:  Loss:     0.0099 Validation Accuracy: 0.989624\n",
      "Epoch 11, MNIST Batch 8:  Loss:     0.0077 Validation Accuracy: 0.991577\n",
      "Epoch 11, MNIST Batch 9:  Loss:     0.0058 Validation Accuracy: 0.991821\n",
      "Epoch 11, MNIST Batch 10:  Loss:     0.0085 Validation Accuracy: 0.992065\n",
      "Epoch 11, MNIST Batch 11:  Loss:     0.0101 Validation Accuracy: 0.991577\n",
      "Epoch 11, MNIST Batch 12:  Loss:     0.0097 Validation Accuracy: 0.990967\n",
      "Epoch 11, MNIST Batch 13:  Loss:     0.0111 Validation Accuracy: 0.991211\n",
      "Epoch 11, MNIST Batch 14:  Loss:     0.0087 Validation Accuracy: 0.991577\n",
      "Epoch 12, MNIST Batch 1:  Loss:     0.0081 Validation Accuracy: 0.991699\n",
      "Epoch 12, MNIST Batch 2:  Loss:     0.0092 Validation Accuracy: 0.992188\n",
      "Epoch 12, MNIST Batch 3:  Loss:     0.0074 Validation Accuracy: 0.992554\n",
      "Epoch 12, MNIST Batch 4:  Loss:     0.0094 Validation Accuracy: 0.992065\n",
      "Epoch 12, MNIST Batch 5:  Loss:     0.0063 Validation Accuracy: 0.992554\n",
      "Epoch 12, MNIST Batch 6:  Loss:     0.0071 Validation Accuracy: 0.992920\n",
      "Epoch 12, MNIST Batch 7:  Loss:     0.0057 Validation Accuracy: 0.992432\n",
      "Epoch 12, MNIST Batch 8:  Loss:     0.0045 Validation Accuracy: 0.991699\n",
      "Epoch 12, MNIST Batch 9:  Loss:     0.0073 Validation Accuracy: 0.991821\n",
      "Epoch 12, MNIST Batch 10:  Loss:     0.0072 Validation Accuracy: 0.991577\n",
      "Epoch 12, MNIST Batch 11:  Loss:     0.0056 Validation Accuracy: 0.992676\n",
      "Epoch 12, MNIST Batch 12:  Loss:     0.0089 Validation Accuracy: 0.991333\n",
      "Epoch 12, MNIST Batch 13:  Loss:     0.0075 Validation Accuracy: 0.992798\n",
      "Epoch 12, MNIST Batch 14:  Loss:     0.0074 Validation Accuracy: 0.992676\n",
      "Epoch 13, MNIST Batch 1:  Loss:     0.0091 Validation Accuracy: 0.992188\n",
      "Epoch 13, MNIST Batch 2:  Loss:     0.0067 Validation Accuracy: 0.992798\n",
      "Epoch 13, MNIST Batch 3:  Loss:     0.0041 Validation Accuracy: 0.992920\n",
      "Epoch 13, MNIST Batch 4:  Loss:     0.0059 Validation Accuracy: 0.991211\n",
      "Epoch 13, MNIST Batch 5:  Loss:     0.0061 Validation Accuracy: 0.992920\n",
      "Epoch 13, MNIST Batch 6:  Loss:     0.0034 Validation Accuracy: 0.992310\n",
      "Epoch 13, MNIST Batch 7:  Loss:     0.0054 Validation Accuracy: 0.990723\n",
      "Epoch 13, MNIST Batch 8:  Loss:     0.0043 Validation Accuracy: 0.991455\n",
      "Epoch 13, MNIST Batch 9:  Loss:     0.0039 Validation Accuracy: 0.992676\n",
      "Epoch 13, MNIST Batch 10:  Loss:     0.0092 Validation Accuracy: 0.992310\n",
      "Epoch 13, MNIST Batch 11:  Loss:     0.0069 Validation Accuracy: 0.991211\n",
      "Epoch 13, MNIST Batch 12:  Loss:     0.0084 Validation Accuracy: 0.992065\n",
      "Epoch 13, MNIST Batch 13:  Loss:     0.0056 Validation Accuracy: 0.992432\n",
      "Epoch 13, MNIST Batch 14:  Loss:     0.0058 Validation Accuracy: 0.992554\n",
      "Epoch 14, MNIST Batch 1:  Loss:     0.0055 Validation Accuracy: 0.991455\n",
      "Epoch 14, MNIST Batch 2:  Loss:     0.0053 Validation Accuracy: 0.989502\n",
      "Epoch 14, MNIST Batch 3:  Loss:     0.0070 Validation Accuracy: 0.989746\n",
      "Epoch 14, MNIST Batch 4:  Loss:     0.0043 Validation Accuracy: 0.991455\n",
      "Epoch 14, MNIST Batch 5:  Loss:     0.0068 Validation Accuracy: 0.991211\n",
      "Epoch 14, MNIST Batch 6:  Loss:     0.0055 Validation Accuracy: 0.990356\n",
      "Epoch 14, MNIST Batch 7:  Loss:     0.0061 Validation Accuracy: 0.990723\n",
      "Epoch 14, MNIST Batch 8:  Loss:     0.0044 Validation Accuracy: 0.992432\n",
      "Epoch 14, MNIST Batch 9:  Loss:     0.0046 Validation Accuracy: 0.992676\n",
      "Epoch 14, MNIST Batch 10:  Loss:     0.0047 Validation Accuracy: 0.992065\n",
      "Epoch 14, MNIST Batch 11:  Loss:     0.0039 Validation Accuracy: 0.992065\n",
      "Epoch 14, MNIST Batch 12:  Loss:     0.0042 Validation Accuracy: 0.993286\n",
      "Epoch 14, MNIST Batch 13:  Loss:     0.0061 Validation Accuracy: 0.991699\n",
      "Epoch 14, MNIST Batch 14:  Loss:     0.0052 Validation Accuracy: 0.991821\n",
      "Epoch 15, MNIST Batch 1:  Loss:     0.0037 Validation Accuracy: 0.992310\n",
      "Epoch 15, MNIST Batch 2:  Loss:     0.0039 Validation Accuracy: 0.992065\n",
      "Epoch 15, MNIST Batch 3:  Loss:     0.0043 Validation Accuracy: 0.992554\n",
      "Epoch 15, MNIST Batch 4:  Loss:     0.0026 Validation Accuracy: 0.990356\n",
      "Epoch 15, MNIST Batch 5:  Loss:     0.0030 Validation Accuracy: 0.991089\n",
      "Epoch 15, MNIST Batch 6:  Loss:     0.0030 Validation Accuracy: 0.992432\n",
      "Epoch 15, MNIST Batch 7:  Loss:     0.0043 Validation Accuracy: 0.991211\n",
      "Epoch 15, MNIST Batch 8:  Loss:     0.0037 Validation Accuracy: 0.992188\n",
      "Epoch 15, MNIST Batch 9:  Loss:     0.0044 Validation Accuracy: 0.992554\n",
      "Epoch 15, MNIST Batch 10:  Loss:     0.0048 Validation Accuracy: 0.990845\n",
      "Epoch 15, MNIST Batch 11:  Loss:     0.0031 Validation Accuracy: 0.992065\n",
      "Epoch 15, MNIST Batch 12:  Loss:     0.0031 Validation Accuracy: 0.991455\n",
      "Epoch 15, MNIST Batch 13:  Loss:     0.0014 Validation Accuracy: 0.992065\n",
      "Epoch 15, MNIST Batch 14:  Loss:     0.0020 Validation Accuracy: 0.993408\n",
      "Epoch 16, MNIST Batch 1:  Loss:     0.0037 Validation Accuracy: 0.992798\n",
      "Epoch 16, MNIST Batch 2:  Loss:     0.0031 Validation Accuracy: 0.993408\n",
      "Epoch 16, MNIST Batch 3:  Loss:     0.0017 Validation Accuracy: 0.993408\n",
      "Epoch 16, MNIST Batch 4:  Loss:     0.0018 Validation Accuracy: 0.991943\n",
      "Epoch 16, MNIST Batch 5:  Loss:     0.0023 Validation Accuracy: 0.991821\n",
      "Epoch 16, MNIST Batch 6:  Loss:     0.0040 Validation Accuracy: 0.993042\n",
      "Epoch 16, MNIST Batch 7:  Loss:     0.0049 Validation Accuracy: 0.993164\n",
      "Epoch 16, MNIST Batch 8:  Loss:     0.0027 Validation Accuracy: 0.992554\n",
      "Epoch 16, MNIST Batch 9:  Loss:     0.0026 Validation Accuracy: 0.992310\n",
      "Epoch 16, MNIST Batch 10:  Loss:     0.0022 Validation Accuracy: 0.992188\n",
      "Epoch 16, MNIST Batch 11:  Loss:     0.0018 Validation Accuracy: 0.993042\n",
      "Epoch 16, MNIST Batch 12:  Loss:     0.0016 Validation Accuracy: 0.993042\n",
      "Epoch 16, MNIST Batch 13:  Loss:     0.0026 Validation Accuracy: 0.992798\n",
      "Epoch 16, MNIST Batch 14:  Loss:     0.0017 Validation Accuracy: 0.992432\n",
      "Epoch 17, MNIST Batch 1:  Loss:     0.0022 Validation Accuracy: 0.991577\n",
      "Epoch 17, MNIST Batch 2:  Loss:     0.0044 Validation Accuracy: 0.990723\n",
      "Epoch 17, MNIST Batch 3:  Loss:     0.0015 Validation Accuracy: 0.991455\n",
      "Epoch 17, MNIST Batch 4:  Loss:     0.0023 Validation Accuracy: 0.992188\n",
      "Epoch 17, MNIST Batch 5:  Loss:     0.0022 Validation Accuracy: 0.992432\n",
      "Epoch 17, MNIST Batch 6:  Loss:     0.0017 Validation Accuracy: 0.991455\n",
      "Epoch 17, MNIST Batch 7:  Loss:     0.0017 Validation Accuracy: 0.991577\n",
      "Epoch 17, MNIST Batch 8:  Loss:     0.0018 Validation Accuracy: 0.993286\n",
      "Epoch 17, MNIST Batch 9:  Loss:     0.0016 Validation Accuracy: 0.992310\n",
      "Epoch 17, MNIST Batch 10:  Loss:     0.0019 Validation Accuracy: 0.992920\n",
      "Epoch 17, MNIST Batch 11:  Loss:     0.0011 Validation Accuracy: 0.992920\n",
      "Epoch 17, MNIST Batch 12:  Loss:     0.0013 Validation Accuracy: 0.993042\n",
      "Epoch 17, MNIST Batch 13:  Loss:     0.0011 Validation Accuracy: 0.992310\n",
      "Epoch 17, MNIST Batch 14:  Loss:     0.0017 Validation Accuracy: 0.993042\n",
      "Epoch 18, MNIST Batch 1:  Loss:     0.0012 Validation Accuracy: 0.992554\n",
      "Epoch 18, MNIST Batch 2:  Loss:     0.0010 Validation Accuracy: 0.991943\n",
      "Epoch 18, MNIST Batch 3:  Loss:     0.0020 Validation Accuracy: 0.991821\n",
      "Epoch 18, MNIST Batch 4:  Loss:     0.0012 Validation Accuracy: 0.992310\n",
      "Epoch 18, MNIST Batch 5:  Loss:     0.0012 Validation Accuracy: 0.992188\n",
      "Epoch 18, MNIST Batch 6:  Loss:     0.0012 Validation Accuracy: 0.992676\n",
      "Epoch 18, MNIST Batch 7:  Loss:     0.0034 Validation Accuracy: 0.993042\n",
      "Epoch 18, MNIST Batch 8:  Loss:     0.0027 Validation Accuracy: 0.992676\n",
      "Epoch 18, MNIST Batch 9:  Loss:     0.0012 Validation Accuracy: 0.992798\n",
      "Epoch 18, MNIST Batch 10:  Loss:     0.0008 Validation Accuracy: 0.992676\n",
      "Epoch 18, MNIST Batch 11:  Loss:     0.0014 Validation Accuracy: 0.992188\n",
      "Epoch 18, MNIST Batch 12:  Loss:     0.0009 Validation Accuracy: 0.992188\n",
      "Epoch 18, MNIST Batch 13:  Loss:     0.0015 Validation Accuracy: 0.993164\n",
      "Epoch 18, MNIST Batch 14:  Loss:     0.0009 Validation Accuracy: 0.992310\n",
      "Epoch 19, MNIST Batch 1:  Loss:     0.0021 Validation Accuracy: 0.992676\n",
      "Epoch 19, MNIST Batch 2:  Loss:     0.0008 Validation Accuracy: 0.992188\n",
      "Epoch 19, MNIST Batch 3:  Loss:     0.0007 Validation Accuracy: 0.992188\n",
      "Epoch 19, MNIST Batch 4:  Loss:     0.0028 Validation Accuracy: 0.992310\n",
      "Epoch 19, MNIST Batch 5:  Loss:     0.0012 Validation Accuracy: 0.991699\n",
      "Epoch 19, MNIST Batch 6:  Loss:     0.0021 Validation Accuracy: 0.992920\n",
      "Epoch 19, MNIST Batch 7:  Loss:     0.0011 Validation Accuracy: 0.992554\n",
      "Epoch 19, MNIST Batch 8:  Loss:     0.0018 Validation Accuracy: 0.992920\n",
      "Epoch 19, MNIST Batch 9:  Loss:     0.0009 Validation Accuracy: 0.992065\n",
      "Epoch 19, MNIST Batch 10:  Loss:     0.0019 Validation Accuracy: 0.991821\n",
      "Epoch 19, MNIST Batch 11:  Loss:     0.0007 Validation Accuracy: 0.991699\n",
      "Epoch 19, MNIST Batch 12:  Loss:     0.0022 Validation Accuracy: 0.991943\n",
      "Epoch 19, MNIST Batch 13:  Loss:     0.0016 Validation Accuracy: 0.992798\n",
      "Epoch 19, MNIST Batch 14:  Loss:     0.0011 Validation Accuracy: 0.992065\n",
      "Epoch 20, MNIST Batch 1:  Loss:     0.0008 Validation Accuracy: 0.992188\n",
      "Epoch 20, MNIST Batch 2:  Loss:     0.0013 Validation Accuracy: 0.992920\n",
      "Epoch 20, MNIST Batch 3:  Loss:     0.0011 Validation Accuracy: 0.992554\n",
      "Epoch 20, MNIST Batch 4:  Loss:     0.0009 Validation Accuracy: 0.993042\n",
      "Epoch 20, MNIST Batch 5:  Loss:     0.0006 Validation Accuracy: 0.993042\n",
      "Epoch 20, MNIST Batch 6:  Loss:     0.0014 Validation Accuracy: 0.992798\n",
      "Epoch 20, MNIST Batch 7:  Loss:     0.0013 Validation Accuracy: 0.992676\n",
      "Epoch 20, MNIST Batch 8:  Loss:     0.0009 Validation Accuracy: 0.991821\n",
      "Epoch 20, MNIST Batch 9:  Loss:     0.0009 Validation Accuracy: 0.991699\n",
      "Epoch 20, MNIST Batch 10:  Loss:     0.0010 Validation Accuracy: 0.991333\n",
      "Epoch 20, MNIST Batch 11:  Loss:     0.0010 Validation Accuracy: 0.991943\n",
      "Epoch 20, MNIST Batch 12:  Loss:     0.0009 Validation Accuracy: 0.991821\n",
      "Epoch 20, MNIST Batch 13:  Loss:     0.0012 Validation Accuracy: 0.991943\n",
      "Epoch 20, MNIST Batch 14:  Loss:     0.0014 Validation Accuracy: 0.991821\n",
      "MNIST accuracy on test set: 0.9916178584098816 ./model\n"
     ]
    }
   ],
   "source": [
    "save_model_path = './model'\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        batches = mnist.train.images.shape[0] // batch_size +1\n",
    "\n",
    "        for i in range(1, batches+1):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            train_features_batch = batch[0].reshape( [-1, 28,28,1])\n",
    "            train_labels_batch = batch[1]\n",
    "            train_nn(sess, optimizer, feature_batch = train_features_batch, label_batch = train_labels_batch, keep_probility=keep_probility)\n",
    "            print('Epoch {:>2}, MNIST Batch {}:  '.format(epoch + 1, i), end='')\n",
    "            print_status(sess, train_features_batch,train_labels_batch, cost, accuracy )\n",
    "            \n",
    "#   在测试集上进行测试       \n",
    "    acc = []\n",
    "    test_batch_size = 4096\n",
    "    test_batches_num = (mnist.test.images.shape[0] // test_batch_size) +1\n",
    "    for i in range(1, test_batches_num+1):\n",
    "        batch = mnist.test.next_batch(test_batch_size)\n",
    "        test_features_batch = batch[0].reshape( [-1, 28,28,1])\n",
    "        test_labels_batch = batch[1]\n",
    "        \n",
    "        acc.append(sess.run(accuracy, feed_dict={x: test_features_batch, y: test_labels_batch, keep_prob:1} ))\n",
    "    acc = np.mean(acc)\n",
    "    print('MNIST accuracy on test set: {} '.format(acc), end='')\n",
    "    \n",
    "    # 保存模型\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)\n",
    "    print(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 在测试集上测试模型"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 33,
>>>>>>> ec58ac9e310756f4cba8f4938cc77f7d9a63c29b
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "(4096, 28, 28, 1)\n",
      "(4096, 10)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-883560ba8583>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_features_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_labels_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0macc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mloaded_x\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtest_features_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloaded_y\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtest_labels_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloaded_keep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda2\\envs\\dlnd\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda2\\envs\\dlnd\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda2\\envs\\dlnd\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda2\\envs\\dlnd\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda2\\envs\\dlnd\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
=======
      "MNIST accuracy on test set: 0.1116536483168602 "
>>>>>>> ec58ac9e310756f4cba8f4938cc77f7d9a63c29b
     ]
    }
   ],
   "source": [
    "save_model_path = './model'\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph = loaded_graph) as sess:\n",
    "    loader = tf.train.import_meta_graph(save_model_path+ '.meta')\n",
    "    loader.restore(sess, save_model_path)\n",
    "    \n",
    "    loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "    loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "    loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "    loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "    \n",
    "    acc = []\n",
    "    batch_size = 4096\n",
    "    batches = (mnist.test.images.shape[0] // batch_size) +1\n",
    "    for i in range(1, batches+1):\n",
    "        batch = mnist.test.next_batch(batch_size)\n",
    "        test_features_batch = batch[0].reshape( [-1, 28,28,1])\n",
    "        test_labels_batch = batch[1]\n",
    "        print(test_features_batch.shape)\n",
    "        print(test_labels_batch.shape)\n",
    "        acc.append(sess.run(loaded_acc, feed_dict={loaded_x: test_features_batch, loaded_y: test_labels_batch, loaded_keep_prob:1} )) #, \n",
    "    print(acc)\n",
    "    acc = np.mean(acc)\n",
    "    print('MNIST accuracy on test set: {} '.format(acc), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.5.4"
=======
   "version": "3.5.2"
>>>>>>> ec58ac9e310756f4cba8f4938cc77f7d9a63c29b
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
