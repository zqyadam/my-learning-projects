{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST_with_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 关于数据集\n",
    "\n",
    "MNIST数据集中包含了三个部分：55000个训练集（mnist.train），10000个测试集（mnist.test）和5000个验证集（mnist.validation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 绘制前10个训练集图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADTCAYAAACRDeixAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEhFq0EVEIkINuohIRKhBFxGJiP8HozAbknnIOjsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8f5cc1de80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 3 4 6 1 8 1 0 9 8]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    image = mnist.train.images[i]\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.imshow(image.reshape([28,28]), cmap='gray_r')\n",
    "plt.show()\n",
    "print(np.argmax(mnist.train.labels[:10], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 28, 28, 1)\n",
      "(5000, 10)\n"
     ]
    }
   ],
   "source": [
    "val_features = np.array([ mnist.validation.images[i].reshape((28,28,1))  for i in range(mnist.validation.images.shape[0])])\n",
    "print(val_features.shape)\n",
    "val_labels = mnist.validation.labels\n",
    "print(val_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建卷积神经网络\n",
    "\n",
    "使用3个卷积层+池化层，2个全连接层构建神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 权重初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weight(shape):\n",
    "    init = tf.truncated_normal(shape, mean=0,  stddev=0.1)\n",
    "    return tf.Variable(init)\n",
    "\n",
    "# 偏置初始化\n",
    "def init_bias(shape):\n",
    "    init = tf.zeros(shape = shape)\n",
    "    return tf.Variable(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注：**这里需要注意权重初始化时的stddev，权重在初始化的时候越靠近0越好，因为当权重全部为0的时候，意味着所有的反向传播值都是一样的，也就是说神经网络没有从误差中学到任何东西，也就无法体现这些超参数的差异化；如果初始化的值远离0了，那么可能会使神经网络在初始化的时候出面对某种结果的喜好，从而影响网络的训练，这种情况可能会使网络陷入局部解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建各个层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 卷积层和池化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 卷积层\n",
    "def conv2d(x_tensor, outputs_num, kernel_size=(3,3), strides=(1,1,1,1) ):\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    weight = init_weight([kernel_size[0], kernel_size[1], shape[3], outputs_num])\n",
    "    bias = init_bias([outputs_num])\n",
    "    \n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weight, strides, padding='SAME' )\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    return conv_layer\n",
    "\n",
    "# 池化层\n",
    "def pooling(x_tensor, kernel_size=(2,2), strides=(1,2,2,1)):\n",
    "    pooling_layer = tf.nn.max_pool(x_tensor, ksize=[1, kernel_size[0],kernel_size[1],1], strides=strides, padding=\"SAME\")\n",
    "    return pooling_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 扁化层\n",
    "扁化层是用来把最后一个卷积池化层的输出转换成单一向量，用来作为后面全连接层的输入层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(x_tensor):\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    x_tensor = tf.reshape(x_tensor, [-1, shape[1]*shape[2]*shape[3]])\n",
    "    return x_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, num_outputs, activation=tf.nn.relu):\n",
    "    return tf.layers.dense(x_tensor, num_outputs, activation=activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建全局神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(x, keep_probility):\n",
    "    \n",
    "    conv_layer1 = conv2d(x, 64)\n",
    "    pooling_layer1 = pooling(conv_layer1)\n",
    "    \n",
    "    conv_layer2 = conv2d(pooling_layer1, 128)\n",
    "    pooling_layer2 = pooling(conv_layer2)\n",
    "\n",
    "    conv_layer3 = conv2d(pooling_layer2, 256)\n",
    "    pooling_layer2 = pooling(conv_layer3)\n",
    "    \n",
    "    flatten_layer = flatten(conv_layer3)\n",
    "    \n",
    "    fully_conn_layer1 = fully_conn(flatten_layer, 256)\n",
    "    fully_conn_layer1 = tf.nn.dropout(fully_conn_layer1, keep_prob = keep_probility)\n",
    "    fully_conn_layer2 = fully_conn(fully_conn_layer1, 128)\n",
    "    fully_conn_layer2 = tf.nn.dropout(fully_conn_layer2, keep_prob = keep_probility)\n",
    "    \n",
    "    output = fully_conn(fully_conn_layer2, 10, activation=None)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name=\"x\")\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10], name=\"y\")\n",
    "keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "logits = conv_net(x, keep_prob)\n",
    "logits = tf.identity(logits, name='logits')\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32),name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_nn(session, optimizer, feature_batch, label_batch, keep_probility):\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probility})\n",
    "\n",
    "def print_status(session, feature_batch, label_batch, cost, accuracy):\n",
    "    loss = session.run(cost, feed_dict={ x: feature_batch, y: label_batch , keep_prob: 1 })\n",
    "    \n",
    "    acc = []\n",
    "    batch_size=4096\n",
    "    batches = mnist.validation.images.shape[0] // batch_size +1\n",
    "    for i in range(1, batches+1):\n",
    "        val_batch = mnist.validation.next_batch(batch_size)\n",
    "        val_features_batch = val_batch[0].reshape( [-1, 28,28,1])\n",
    "        val_labels_batch = val_batch[1]\n",
    "        acc.append( session.run(accuracy, feed_dict={x : val_features_batch, y: val_labels_batch, keep_prob:1}))\n",
    "    acc = np.mean(acc)\n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss,acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size=4096\n",
    "keep_probility = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, MNIST Batch 1:  Loss:     2.0209 Validation Accuracy: 0.303345\n",
      "Epoch  1, MNIST Batch 2:  Loss:     1.7948 Validation Accuracy: 0.481445\n",
      "Epoch  1, MNIST Batch 3:  Loss:     1.4333 Validation Accuracy: 0.657593\n",
      "Epoch  1, MNIST Batch 4:  Loss:     1.1217 Validation Accuracy: 0.717285\n",
      "Epoch  1, MNIST Batch 5:  Loss:     0.9481 Validation Accuracy: 0.689819\n",
      "Epoch  1, MNIST Batch 6:  Loss:     0.7462 Validation Accuracy: 0.792725\n",
      "Epoch  1, MNIST Batch 7:  Loss:     0.6289 Validation Accuracy: 0.831909\n",
      "Epoch  1, MNIST Batch 8:  Loss:     0.4952 Validation Accuracy: 0.872559\n",
      "Epoch  1, MNIST Batch 9:  Loss:     0.4357 Validation Accuracy: 0.880127\n",
      "Epoch  1, MNIST Batch 10:  Loss:     0.3577 Validation Accuracy: 0.894653\n",
      "Epoch  1, MNIST Batch 11:  Loss:     0.3578 Validation Accuracy: 0.902466\n",
      "Epoch  1, MNIST Batch 12:  Loss:     0.3127 Validation Accuracy: 0.912964\n",
      "Epoch  1, MNIST Batch 13:  Loss:     0.2752 Validation Accuracy: 0.921143\n",
      "Epoch  1, MNIST Batch 14:  Loss:     0.2711 Validation Accuracy: 0.930176\n",
      "Epoch  2, MNIST Batch 1:  Loss:     0.2363 Validation Accuracy: 0.936157\n",
      "Epoch  2, MNIST Batch 2:  Loss:     0.2212 Validation Accuracy: 0.941040\n",
      "Epoch  2, MNIST Batch 3:  Loss:     0.2140 Validation Accuracy: 0.946411\n",
      "Epoch  2, MNIST Batch 4:  Loss:     0.1904 Validation Accuracy: 0.950684\n",
      "Epoch  2, MNIST Batch 5:  Loss:     0.1994 Validation Accuracy: 0.952026\n",
      "Epoch  2, MNIST Batch 6:  Loss:     0.1731 Validation Accuracy: 0.955688\n",
      "Epoch  2, MNIST Batch 7:  Loss:     0.1603 Validation Accuracy: 0.957642\n",
      "Epoch  2, MNIST Batch 8:  Loss:     0.1528 Validation Accuracy: 0.957275\n",
      "Epoch  2, MNIST Batch 9:  Loss:     0.1364 Validation Accuracy: 0.961304\n",
      "Epoch  2, MNIST Batch 10:  Loss:     0.1324 Validation Accuracy: 0.966187\n",
      "Epoch  2, MNIST Batch 11:  Loss:     0.1383 Validation Accuracy: 0.966309\n",
      "Epoch  2, MNIST Batch 12:  Loss:     0.1127 Validation Accuracy: 0.966553\n",
      "Epoch  2, MNIST Batch 13:  Loss:     0.1312 Validation Accuracy: 0.968872\n",
      "Epoch  2, MNIST Batch 14:  Loss:     0.1098 Validation Accuracy: 0.970459\n",
      "Epoch  3, MNIST Batch 1:  Loss:     0.1068 Validation Accuracy: 0.972656\n",
      "Epoch  3, MNIST Batch 2:  Loss:     0.0928 Validation Accuracy: 0.975342\n",
      "Epoch  3, MNIST Batch 3:  Loss:     0.0884 Validation Accuracy: 0.977295\n",
      "Epoch  3, MNIST Batch 4:  Loss:     0.0712 Validation Accuracy: 0.975464\n",
      "Epoch  3, MNIST Batch 5:  Loss:     0.0904 Validation Accuracy: 0.972900\n",
      "Epoch  3, MNIST Batch 6:  Loss:     0.0835 Validation Accuracy: 0.976685\n",
      "Epoch  3, MNIST Batch 7:  Loss:     0.0692 Validation Accuracy: 0.974976\n",
      "Epoch  3, MNIST Batch 8:  Loss:     0.0916 Validation Accuracy: 0.976562\n",
      "Epoch  3, MNIST Batch 9:  Loss:     0.0839 Validation Accuracy: 0.976562\n",
      "Epoch  3, MNIST Batch 10:  Loss:     0.0734 Validation Accuracy: 0.979004\n",
      "Epoch  3, MNIST Batch 11:  Loss:     0.0875 Validation Accuracy: 0.979248\n",
      "Epoch  3, MNIST Batch 12:  Loss:     0.0799 Validation Accuracy: 0.977417\n",
      "Epoch  3, MNIST Batch 13:  Loss:     0.0659 Validation Accuracy: 0.980835\n",
      "Epoch  3, MNIST Batch 14:  Loss:     0.0699 Validation Accuracy: 0.981689\n",
      "Epoch  4, MNIST Batch 1:  Loss:     0.0752 Validation Accuracy: 0.981201\n",
      "Epoch  4, MNIST Batch 2:  Loss:     0.0645 Validation Accuracy: 0.980225\n",
      "Epoch  4, MNIST Batch 3:  Loss:     0.0525 Validation Accuracy: 0.979248\n",
      "Epoch  4, MNIST Batch 4:  Loss:     0.0481 Validation Accuracy: 0.980469\n",
      "Epoch  4, MNIST Batch 5:  Loss:     0.0561 Validation Accuracy: 0.982666\n",
      "Epoch  4, MNIST Batch 6:  Loss:     0.0672 Validation Accuracy: 0.981079\n",
      "Epoch  4, MNIST Batch 7:  Loss:     0.0616 Validation Accuracy: 0.983765\n",
      "Epoch  4, MNIST Batch 8:  Loss:     0.0460 Validation Accuracy: 0.982544\n",
      "Epoch  4, MNIST Batch 9:  Loss:     0.0555 Validation Accuracy: 0.983276\n",
      "Epoch  4, MNIST Batch 10:  Loss:     0.0488 Validation Accuracy: 0.982910\n",
      "Epoch  4, MNIST Batch 11:  Loss:     0.0491 Validation Accuracy: 0.981323\n",
      "Epoch  4, MNIST Batch 12:  Loss:     0.0579 Validation Accuracy: 0.984497\n",
      "Epoch  4, MNIST Batch 13:  Loss:     0.0476 Validation Accuracy: 0.984741\n",
      "Epoch  4, MNIST Batch 14:  Loss:     0.0403 Validation Accuracy: 0.985962\n",
      "Epoch  5, MNIST Batch 1:  Loss:     0.0435 Validation Accuracy: 0.983887\n",
      "Epoch  5, MNIST Batch 2:  Loss:     0.0399 Validation Accuracy: 0.985596\n",
      "Epoch  5, MNIST Batch 3:  Loss:     0.0449 Validation Accuracy: 0.986328\n",
      "Epoch  5, MNIST Batch 4:  Loss:     0.0533 Validation Accuracy: 0.984375\n",
      "Epoch  5, MNIST Batch 5:  Loss:     0.0400 Validation Accuracy: 0.985962\n",
      "Epoch  5, MNIST Batch 6:  Loss:     0.0474 Validation Accuracy: 0.984863\n",
      "Epoch  5, MNIST Batch 7:  Loss:     0.0441 Validation Accuracy: 0.986450\n",
      "Epoch  5, MNIST Batch 8:  Loss:     0.0456 Validation Accuracy: 0.985229\n",
      "Epoch  5, MNIST Batch 9:  Loss:     0.0328 Validation Accuracy: 0.986694\n",
      "Epoch  5, MNIST Batch 10:  Loss:     0.0416 Validation Accuracy: 0.987427\n",
      "Epoch  5, MNIST Batch 11:  Loss:     0.0381 Validation Accuracy: 0.988281\n",
      "Epoch  5, MNIST Batch 12:  Loss:     0.0305 Validation Accuracy: 0.987549\n",
      "Epoch  5, MNIST Batch 13:  Loss:     0.0340 Validation Accuracy: 0.987671\n",
      "Epoch  5, MNIST Batch 14:  Loss:     0.0353 Validation Accuracy: 0.985840\n",
      "Epoch  6, MNIST Batch 1:  Loss:     0.0331 Validation Accuracy: 0.985107\n",
      "Epoch  6, MNIST Batch 2:  Loss:     0.0417 Validation Accuracy: 0.986938\n",
      "Epoch  6, MNIST Batch 3:  Loss:     0.0389 Validation Accuracy: 0.987305\n",
      "Epoch  6, MNIST Batch 4:  Loss:     0.0397 Validation Accuracy: 0.985962\n",
      "Epoch  6, MNIST Batch 5:  Loss:     0.0314 Validation Accuracy: 0.988403\n",
      "Epoch  6, MNIST Batch 6:  Loss:     0.0396 Validation Accuracy: 0.985962\n",
      "Epoch  6, MNIST Batch 7:  Loss:     0.0388 Validation Accuracy: 0.987183\n",
      "Epoch  6, MNIST Batch 8:  Loss:     0.0367 Validation Accuracy: 0.989258\n",
      "Epoch  6, MNIST Batch 9:  Loss:     0.0283 Validation Accuracy: 0.989014\n",
      "Epoch  6, MNIST Batch 10:  Loss:     0.0432 Validation Accuracy: 0.987305\n",
      "Epoch  6, MNIST Batch 11:  Loss:     0.0230 Validation Accuracy: 0.988037\n",
      "Epoch  6, MNIST Batch 12:  Loss:     0.0256 Validation Accuracy: 0.987427\n",
      "Epoch  6, MNIST Batch 13:  Loss:     0.0282 Validation Accuracy: 0.988037\n",
      "Epoch  6, MNIST Batch 14:  Loss:     0.0310 Validation Accuracy: 0.987793\n",
      "Epoch  7, MNIST Batch 1:  Loss:     0.0276 Validation Accuracy: 0.987549\n",
      "Epoch  7, MNIST Batch 2:  Loss:     0.0347 Validation Accuracy: 0.988770\n",
      "Epoch  7, MNIST Batch 3:  Loss:     0.0357 Validation Accuracy: 0.988159\n",
      "Epoch  7, MNIST Batch 4:  Loss:     0.0253 Validation Accuracy: 0.988892\n",
      "Epoch  7, MNIST Batch 5:  Loss:     0.0246 Validation Accuracy: 0.989136\n",
      "Epoch  7, MNIST Batch 6:  Loss:     0.0265 Validation Accuracy: 0.987671\n",
      "Epoch  7, MNIST Batch 7:  Loss:     0.0317 Validation Accuracy: 0.989014\n",
      "Epoch  7, MNIST Batch 8:  Loss:     0.0272 Validation Accuracy: 0.988892\n",
      "Epoch  7, MNIST Batch 9:  Loss:     0.0260 Validation Accuracy: 0.988647\n",
      "Epoch  7, MNIST Batch 10:  Loss:     0.0187 Validation Accuracy: 0.989380\n",
      "Epoch  7, MNIST Batch 11:  Loss:     0.0269 Validation Accuracy: 0.988647\n",
      "Epoch  7, MNIST Batch 12:  Loss:     0.0234 Validation Accuracy: 0.989502\n",
      "Epoch  7, MNIST Batch 13:  Loss:     0.0234 Validation Accuracy: 0.989624\n",
      "Epoch  7, MNIST Batch 14:  Loss:     0.0194 Validation Accuracy: 0.989746\n",
      "Epoch  8, MNIST Batch 1:  Loss:     0.0223 Validation Accuracy: 0.990723\n",
      "Epoch  8, MNIST Batch 2:  Loss:     0.0246 Validation Accuracy: 0.990234\n",
      "Epoch  8, MNIST Batch 3:  Loss:     0.0186 Validation Accuracy: 0.989014\n",
      "Epoch  8, MNIST Batch 4:  Loss:     0.0196 Validation Accuracy: 0.991089\n",
      "Epoch  8, MNIST Batch 5:  Loss:     0.0209 Validation Accuracy: 0.989746\n",
      "Epoch  8, MNIST Batch 6:  Loss:     0.0242 Validation Accuracy: 0.990234\n",
      "Epoch  8, MNIST Batch 7:  Loss:     0.0279 Validation Accuracy: 0.990601\n",
      "Epoch  8, MNIST Batch 8:  Loss:     0.0178 Validation Accuracy: 0.989990\n",
      "Epoch  8, MNIST Batch 9:  Loss:     0.0224 Validation Accuracy: 0.991699\n",
      "Epoch  8, MNIST Batch 10:  Loss:     0.0203 Validation Accuracy: 0.989990\n",
      "Epoch  8, MNIST Batch 11:  Loss:     0.0202 Validation Accuracy: 0.989380\n",
      "Epoch  8, MNIST Batch 12:  Loss:     0.0173 Validation Accuracy: 0.990234\n",
      "Epoch  8, MNIST Batch 13:  Loss:     0.0141 Validation Accuracy: 0.990723\n",
      "Epoch  8, MNIST Batch 14:  Loss:     0.0185 Validation Accuracy: 0.990112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9, MNIST Batch 1:  Loss:     0.0170 Validation Accuracy: 0.989868\n",
      "Epoch  9, MNIST Batch 2:  Loss:     0.0161 Validation Accuracy: 0.990723\n",
      "Epoch  9, MNIST Batch 3:  Loss:     0.0215 Validation Accuracy: 0.991699\n",
      "Epoch  9, MNIST Batch 4:  Loss:     0.0255 Validation Accuracy: 0.989990\n",
      "Epoch  9, MNIST Batch 5:  Loss:     0.0164 Validation Accuracy: 0.990356\n",
      "Epoch  9, MNIST Batch 6:  Loss:     0.0210 Validation Accuracy: 0.990723\n",
      "Epoch  9, MNIST Batch 7:  Loss:     0.0203 Validation Accuracy: 0.990356\n",
      "Epoch  9, MNIST Batch 8:  Loss:     0.0161 Validation Accuracy: 0.990479\n",
      "Epoch  9, MNIST Batch 9:  Loss:     0.0169 Validation Accuracy: 0.990967\n",
      "Epoch  9, MNIST Batch 10:  Loss:     0.0152 Validation Accuracy: 0.991089\n",
      "Epoch  9, MNIST Batch 11:  Loss:     0.0136 Validation Accuracy: 0.991943\n",
      "Epoch  9, MNIST Batch 12:  Loss:     0.0163 Validation Accuracy: 0.991577\n",
      "Epoch  9, MNIST Batch 13:  Loss:     0.0144 Validation Accuracy: 0.992188\n",
      "Epoch  9, MNIST Batch 14:  Loss:     0.0133 Validation Accuracy: 0.991821\n",
      "Epoch 10, MNIST Batch 1:  Loss:     0.0183 Validation Accuracy: 0.991577\n",
      "Epoch 10, MNIST Batch 2:  Loss:     0.0177 Validation Accuracy: 0.990601\n",
      "Epoch 10, MNIST Batch 3:  Loss:     0.0221 Validation Accuracy: 0.990479\n",
      "Epoch 10, MNIST Batch 4:  Loss:     0.0143 Validation Accuracy: 0.991455\n",
      "Epoch 10, MNIST Batch 5:  Loss:     0.0145 Validation Accuracy: 0.991577\n",
      "Epoch 10, MNIST Batch 6:  Loss:     0.0163 Validation Accuracy: 0.990479\n",
      "Epoch 10, MNIST Batch 7:  Loss:     0.0190 Validation Accuracy: 0.992554\n",
      "Epoch 10, MNIST Batch 8:  Loss:     0.0108 Validation Accuracy: 0.991943\n",
      "Epoch 10, MNIST Batch 9:  Loss:     0.0174 Validation Accuracy: 0.992432\n",
      "Epoch 10, MNIST Batch 10:  Loss:     0.0167 Validation Accuracy: 0.991333\n",
      "Epoch 10, MNIST Batch 11:  Loss:     0.0145 Validation Accuracy: 0.989502\n",
      "Epoch 10, MNIST Batch 12:  Loss:     0.0150 Validation Accuracy: 0.990723\n",
      "Epoch 10, MNIST Batch 13:  Loss:     0.0155 Validation Accuracy: 0.989258\n",
      "Epoch 10, MNIST Batch 14:  Loss:     0.0152 Validation Accuracy: 0.992920\n",
      "MNIST accuracy on test set: 0.9913737177848816 ./model\n"
     ]
    }
   ],
   "source": [
    "save_model_path = './model'\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        batches = mnist.train.images.shape[0] // batch_size +1\n",
    "\n",
    "        for i in range(1, batches+1):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            train_features_batch = batch[0].reshape( [-1, 28,28,1])\n",
    "            train_labels_batch = batch[1]\n",
    "            train_nn(sess, optimizer, feature_batch = train_features_batch, label_batch = train_labels_batch, keep_probility=keep_probility)\n",
    "            print('Epoch {:>2}, MNIST Batch {}:  '.format(epoch + 1, i), end='')\n",
    "            print_status(sess, train_features_batch,train_labels_batch, cost, accuracy )\n",
    "            \n",
    "#   在测试集上进行测试       \n",
    "    acc = []\n",
    "    test_batch_size = 4096\n",
    "    test_batches_num = (mnist.test.images.shape[0] // test_batch_size) +1\n",
    "    for i in range(1, test_batches_num+1):\n",
    "        batch = mnist.test.next_batch(test_batch_size)\n",
    "        test_features_batch = batch[0].reshape( [-1, 28,28,1])\n",
    "        test_labels_batch = batch[1]\n",
    "        \n",
    "        acc.append(sess.run(accuracy, feed_dict={x: test_features_batch, y: test_labels_batch, keep_prob:1} ))\n",
    "    acc = np.mean(acc)\n",
    "    print('MNIST accuracy on test set: {} '.format(acc), end='')\n",
    "    \n",
    "    # 保存模型\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)\n",
    "    print(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 在测试集上测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keep_prob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d2aa7c0eb919>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mtest_labels_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0macc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mloaded_x\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtest_features_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloaded_y\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtest_labels_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#, loaded_keep_prob:1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'MNIST accuracy on test set: {} '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keep_prob' is not defined"
     ]
    }
   ],
   "source": [
    "save_model_path = './model'\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph = loaded_graph) as sess:\n",
    "    loader = tf.train.import_meta_graph(save_model_path+ '.meta')\n",
    "    loader.restore(sess, save_model_path)\n",
    "    \n",
    "    loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "    loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "    loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "    loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "    \n",
    "    acc = []\n",
    "    batch_size = 256\n",
    "    batches = (mnist.test.images.shape[0] // batch_size) +1\n",
    "    for i in range(1, batches+1):\n",
    "        batch = mnist.test.next_batch(batch_size)\n",
    "        test_features_batch = batch[0].reshape( [-1, 28,28,1])\n",
    "        test_labels_batch = batch[1]\n",
    "        \n",
    "        acc.append(sess.run(loaded_acc, feed_dict={loaded_x: test_features_batch, loaded_y: test_labels_batch, loaded_keep_prob:1} )) #, \n",
    "    acc = np.mean(acc)\n",
    "    print('MNIST accuracy on test set: {} '.format(acc), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlnd",
   "language": "python",
   "name": "dlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
